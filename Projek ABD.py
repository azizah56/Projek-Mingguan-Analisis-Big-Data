# -*- coding: utf-8 -*-
"""Kelompok6_Praktikum_ABD_6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IvN_wFHOM55uVy6Vxs7a8PNpgVfyzzD3

Anggota Kelompok :
1. Muhammad Dafha Syahrizal
2. Siti Nur Azizah
3. Adisya Ridia Nurahma
4. Lis Nurani
"""

!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q http://archive.apache.org/dist/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz
!tar xf spark-3.3.2-bin-hadoop3.tgz
!pip install -q findspark

import os # mengimport modul os untuk berinteraksi dengan sistem os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64" # menentukan direktori JAVA_HOME yang akan digunakan oleh pyspark
os.environ["SPARK_HOME"] = "/content/spark-3.3.2-bin-hadoop3" # enentukan direktori SPARK_HOME yang akan digunakan oleh pyspark

!pip install pyspark # menginstall pyspark dengan menggunakan pip

import findspark # mengimport modul findspark untuk menemukan spark
findspark.init() # menemukan spark dan memperbarui PATH environtment
print('[Done] You can use pyspark now.') # mencetak pesan

from pyspark.sql import SparkSession # mengimpor kelas SparkSession dari modul pyspark.sql. SparkSession adalah pintu masuk utama ke fitur-fitur pengolahan data di Spark
from pyspark.sql.session import SparkSession # mengimpor kelas SparkSession dari modul pyspark.sql.session. Baris ini sebenarnya tidak diperlukan karena kelas SparkSession sudah diimpor pada baris sebelumnya
from pyspark.sql.functions import when # mengimpor fungsi when dari modul pyspark.sql.functions. Fungsi ini digunakan untuk membuat kondisi logika dalam data frame
from pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer # mengimpor kelas VectorAssembler, StandardScaler, dan StringIndexer dari modul pyspark.ml.feature. Kelas-kelas ini digunakan untuk mempersiapkan fitur-fitur untuk model machine learning
from pyspark.ml.classification import LogisticRegression # mengimpor kelas LogisticRegression dari modul pyspark.ml.classification. Kelas ini digunakan untuk membuat model klasifikasi logistik
from pyspark.ml.evaluation import BinaryClassificationEvaluator # mengimpor kelas BinaryClassificationEvaluator dari modul pyspark.ml.evaluation. Kelas ini digunakan untuk mengevaluasi performa model klasifikasi biner

spark = SparkSession.builder \ # membuat objek SparkSession.builder dan menyimpannya ke dalam variabel spark
        .master('local') \ # menentukan mode cluster, di sini mode local digunakan yang artinya Spark akan dijalankan pada mesin lokal
        .appName('Air BnB Price') \ # appName('Air BnB Price') - menentukan nama aplikasi PySpark yang akan digunakan untuk monitoring dan logging
        .getOrCreate() # mengambil atau membuat SparkSession tergantung pada apakah sudah ada SparkSession yang berjalan atau tidak. Jika SparkSession sudah ada, maka objek yang sudah ada akan digunakan, jika tidak, maka objek baru akan dibuat

spark

from google.colab import drive # mengimport modul drive dari google colab
drive.mount('/content/drive') # mengaitkan gdrive dengan google colab

filePath = "/content/drive/MyDrive/Praktikum ABD" # menyimpan path dan direktori

airbnbDF = spark.read.parquet(filePath) # untuk membaca data dari file Parquet yang terletak di path
airbnbDF.select("neighbourhood_cleansed", "room_type", "bedrooms", "bathrooms", "number_of_reviews", "price").show(5)
# untuk memilih kolom tertentu dari dataframe airbnbDF dan menampilkannya pada output dengan memanggil fungsi show(5) untuk menampilkan lima baris pertama dari data

airbnbDF = spark.read.parquet(filePath) # membaca data Airbnb dari file dengan format parquet yang terletak di path atau lokasi file yang ditentukan oleh variable filePath
display(airbnbDF) # menampilkan isi dari variable airbnbDF

airbnbDF.printSchema() # untuk menampilkan skema dari DataFrame airbnbDF

trainDF, testDF = airbnbDF.randomSplit([.8, .2], seed=42) # membagi DataFrame airbnbDF menjadi dua bagian yaitu trainDF dan testDF dengan proporsi 80:20
print(f"""There are {trainDF.count()} rows in the training set,
and {testDF.count()} in the test set""") # mencetak jumlah baris dari trainDF dan testDF

from pyspark.ml.feature import VectorAssembler # mengimport VectorAssembler dari PySpark ML Library
vecAssembler = VectorAssembler(inputCols=["bedrooms"], outputCol="features") # membuat objek vecAssembler yang akan digunakan untuk mengubah kolom bedrooms menjadi vektor fitur
vecTrainDF = vecAssembler.transform(trainDF) # menggunakan vecAssembler untuk mengubah kolom bedrooms di DataFrame trainDF menjadi vektor fitur baru features di DataFrame vecTrainDF
vecTrainDF.select("bedrooms", "features", "price").show(10) # untuk menampilkan 10 baris pertama dari kolom bedrooms, features, dan price di DataFrame vecTrainDF

from pyspark.ml.regression import LinearRegression # kita mengimport LinearRegression dari PySpark ML Library
lr = LinearRegression(featuresCol="features", labelCol="price") # membuat objek lr yang akan digunakan untuk melakukan regresi linier
lrModel = lr.fit(vecTrainDF) # menggunakan objek lr untuk melakukan fit terhadap DataFrame vecTrainDF dan menyimpan hasil fit ke dalam objek lrModel

m = round(lrModel.coefficients[0], 2) #  mengambil nilai koefisien regresi linier dengan menggunakan lrModel.coefficients[0]. Kita membulatkannya menjadi 2 angka desimal menggunakan fungsi round() dan menyimpannya ke dalam variable m
b = round(lrModel.intercept, 2) # mengambil nilai intercept dari model regresi linier dengan menggunakan lrModel.intercept. Kita membulatkannya menjadi 2 angka desimal menggunakan fungsi round() dan menyimpannya ke dalam variable b
print(f"""The formula for the linear regression line is
price = {m}*bedrooms + {b}""")

from pyspark.ml import Pipeline # mengimport Pipeline dari PySpark ML
pipeline = Pipeline(stages=[vecAssembler, lr]) # kita membuat sebuah pipeline dengan memasukkan dua tahap transformasi data kita (vecAssembler) dan model regresi linier (lr) ke dalam sebuah list dan menyimpannya ke dalam variable pipeline
pipelineModel = pipeline.fit(trainDF) # menggunakan objek pipeline untuk melakukan fit terhadap DataFrame trainDF dan menyimpan hasil fit ke dalam objek pipelineModel

predDF = pipelineModel.transform(testDF)
predDF.select("bedrooms", "features", "price", "prediction").show(10)

from pyspark.ml.feature import OneHotEncoder, StringIndexer
categoricalCols = [field for (field, dataType) in trainDF.dtypes
 if dataType == "string"]
indexOutputCols = [x + "Index" for x in categoricalCols]
oheOutputCols = [x + "OHE" for x in categoricalCols]
stringIndexer = StringIndexer(inputCols=categoricalCols,
 outputCols=indexOutputCols,
 handleInvalid="skip")
oheEncoder = OneHotEncoder(inputCols=indexOutputCols,
 outputCols=oheOutputCols)
numericCols = [field for (field, dataType) in trainDF.dtypes
 if ((dataType == "double") & (field != "price"))]
assemblerInputs = oheOutputCols + numericCols
vecAssembler = VectorAssembler(inputCols=assemblerInputs,
 outputCol="features")

from pyspark.ml.feature import RFormula
rFormula = RFormula(formula="price ~ .",
 featuresCol="features",
 labelCol="price",
 handleInvalid="skip")

lr = LinearRegression(labelCol="price", featuresCol="features")
pipeline = Pipeline(stages = [stringIndexer, oheEncoder, vecAssembler, lr])
# Or use RFormula
# pipeline = Pipeline(stages = [rFormula, lr])
pipelineModel = pipeline.fit(trainDF)
predDF = pipelineModel.transform(testDF)
predDF.select("features", "price", "prediction").show(5)

from pyspark.ml.evaluation import RegressionEvaluator
regressionEvaluator = RegressionEvaluator(
 predictionCol="prediction",
 labelCol="price",
 metricName="rmse")
rmse = regressionEvaluator.evaluate(predDF)

"""
regressionEvaluator.evaluate(predDF) merupakan fungsi untuk mengevaluasi kinerja model regresi dengan menggunakan metrik Root Mean Squared Error (RMSE).

dalam hal ini RMSE adalah metrik yang umum digunakan dalam model regresi untuk mengukur kesalahan prediksi dari model. Metrik ini menghitung akar rata-rata kuadrat dari selisih antara nilai aktual dan nilai yang diprediksi oleh model. Semakin kecil nilai RMSE, semakin baik kinerja model, karena nilai RMSE yang kecil menunjukkan bahwa selisih antara nilai aktual dan prediksi model lebih kecil."""

r2 = regressionEvaluator.setMetricName("r2").evaluate(predDF)

"""regressionEvaluator.evaluate(predDF), predDF adalah DataFrame yang berisi prediksi yang dihasilkan oleh model regresi, sedangkan regressionEvaluator adalah evaluator yang digunakan untuk mengevaluasi kinerja model regresi. Setelah dievaluasi, nilai RMSE akan dihasilkan sebagai output dari fungsi ini."""

print(f"RMSE : {rmse:.1f}")
print(f"R2 : {r2}")

"""dari data yang kami gunakan menghasilkan RMSE yaitu sebesar 220.6 dan nilai R2 yaitu sebesar 0.16019847434921997


"""

pipelinePath = "/tmp/lr-pipeline-model"
pipelineModel.write().overwrite().save(pipelinePath)

from pyspark.ml import PipelineModel #sebuah kelas dalam library PySpark yang digunakan untuk menyimpan pipeline yang telah dilatih pada dataset tertentu.
savedPipelineModel = PipelineModel.load(pipelinePath)

from pyspark.ml.regression import DecisionTreeRegressor
dt = DecisionTreeRegressor(labelCol="price")
# Filter for just numeric columns (and exclude price, our label)
numericCols = [field for (field, dataType) in trainDF.dtypes
 if ((dataType == "double") & (field != "price"))]
# Combine output of StringIndexer defined above and numeric columns
assemblerInputs = indexOutputCols + numericCols
vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol="features")
# Combine stages into pipeline
stages = [stringIndexer, vecAssembler, dt]
pipeline = Pipeline(stages=stages)
#pipelineModel = pipeline.fit(trainDF) # This line should error

dt.setMaxBins(40)
pipelineModel = pipeline.fit(trainDF)

dtModel = pipelineModel.stages[-1]
print(dtModel.toDebugString)

import pandas as pd
featureImp = pd.DataFrame(
 list(zip(vecAssembler.getInputCols(), dtModel.featureImportances)),columns=["feature", "importance"])
featureImp.sort_values(by="importance", ascending=False)

from pyspark.ml.regression import RandomForestRegressor
rf = RandomForestRegressor(labelCol="price", maxBins=40, seed=42)

pipeline = Pipeline(stages = [stringIndexer, vecAssembler, rf])

from pyspark.ml.tuning import ParamGridBuilder
paramGrid = (ParamGridBuilder()
 .addGrid(rf.maxDepth, [2, 4, 6])
 .addGrid(rf.numTrees, [10, 100])
 .build())

evaluator = RegressionEvaluator(labelCol="price",
                                predictionCol="prediction",
                                metricName="rmse")

from pyspark.ml.tuning import CrossValidator
cv = CrossValidator(estimator=pipeline,
                    evaluator=evaluator,
                    estimatorParamMaps=paramGrid,
                    numFolds=3,
                    seed=42)
cvModel = cv.fit(trainDF)

list(zip(cvModel.getEstimatorParamMaps(), cvModel.avgMetrics))

cvModel = cv.setParallelism(4).fit(trainDF)

cv = CrossValidator(estimator=rf,
 evaluator=evaluator,
 estimatorParamMaps=paramGrid,
 numFolds=3,
 parallelism=4,
 seed=42)
pipeline = Pipeline(stages=[stringIndexer, vecAssembler, cv])
pipelineModel = pipeline.fit(trainDF)